"""
KV Cache íŒŒì¼ ì¡°íšŒ ìŠ¤í¬ë¦½íŠ¸

ì €ì¥ëœ .pt íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ì–´ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
"""

import torch
import sys
import os


def format_bytes(bytes):
    """Convert bytes to human readable format"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes < 1024.0:
            return f"{bytes:.2f} {unit}"
        bytes /= 1024.0
    return f"{bytes:.2f} TB"


def format_tensor_info(tensor):
    """Format tensor information"""
    if isinstance(tensor, torch.Tensor):
        return f"shape={list(tensor.shape)}, dtype={tensor.dtype}, device={tensor.device}"
    return str(type(tensor))


def inspect_kv_cache(filepath):
    """
    KV Cache íŒŒì¼ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¶œë ¥
    
    Args:
        filepath: .pt íŒŒì¼ ê²½ë¡œ
    """
    
    print("="*80)
    print("KV Cache íŒŒì¼ ì¡°íšŒ")
    print("="*80)
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(filepath):
        print(f"\nâŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        return
    
    # íŒŒì¼ í¬ê¸°
    file_size = os.path.getsize(filepath)
    print(f"\nğŸ“ íŒŒì¼ ì •ë³´:")
    print(f"   ê²½ë¡œ: {filepath}")
    print(f"   í¬ê¸°: {format_bytes(file_size)}")
    
    # íŒŒì¼ ë¡œë“œ
    print(f"\nğŸ“¥ íŒŒì¼ ë¡œë”© ì¤‘...")
    try:
        data = torch.load(filepath, map_location='cpu')
        print(f"   âœ“ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"   âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ìµœìƒìœ„ í‚¤ ì¶œë ¥
    print(f"\nğŸ“‹ ìµœìƒìœ„ êµ¬ì¡°:")
    for key in data.keys():
        print(f"   - {key}")
    
    # Metadata ì¶œë ¥
    if 'metadata' in data:
        print(f"\nğŸ·ï¸  ë©”íƒ€ë°ì´í„°:")
        metadata = data['metadata']
        for key, value in metadata.items():
            print(f"   {key:20s}: {value}")
    
    # Input data ì¶œë ¥
    if 'input_data' in data:
        print(f"\nğŸ“ ì…ë ¥ ë°ì´í„°:")
        input_data = data['input_data']
        
        if 'text' in input_data:
            text = input_data['text']
            preview = text[:100] + "..." if len(text) > 100 else text
            print(f"   í…ìŠ¤íŠ¸: {preview}")
            print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        if 'token_ids' in input_data:
            tokens = input_data['token_ids']
            print(f"   í† í° ìˆ˜: {len(tokens)}")
            print(f"   í† í° ID (ì²˜ìŒ 20ê°œ): {tokens[:20]}")
        
        if 'token_length' in input_data:
            print(f"   í† í° ê¸¸ì´: {input_data['token_length']}")
    
    # Prefill KV Cache ì •ë³´
    if 'prefill_kv' in data:
        print(f"\nğŸ”„ Prefill KV Cache:")
        prefill = data['prefill_kv']
        
        if 'seq_length' in prefill:
            print(f"   ì‹œí€€ìŠ¤ ê¸¸ì´: {prefill['seq_length']}")
        
        if 'attention_mask' in prefill:
            mask = prefill['attention_mask']
            print(f"   Attention Mask: {format_tensor_info(mask)}")
        
        if 'past_key_values' in prefill:
            kv = prefill['past_key_values']
            print(f"   KV Cache êµ¬ì¡°:")
            print(f"     ë ˆì´ì–´ ìˆ˜: {len(kv)}")
            
            if len(kv) > 0:
                # ì²« ë²ˆì§¸ ë ˆì´ì–´ ì •ë³´
                first_layer = kv[0]
                keys, values = first_layer
                print(f"     ê° ë ˆì´ì–´:")
                print(f"       Keys:   {format_tensor_info(keys)}")
                print(f"       Values: {format_tensor_info(values)}")
                
                # ë©”ëª¨ë¦¬ ê³„ì‚°
                key_size = keys.element_size() * keys.nelement()
                value_size = values.element_size() * values.nelement()
                total_per_layer = key_size + value_size
                total_all_layers = total_per_layer * len(kv)
                
                print(f"     ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
                print(f"       ë ˆì´ì–´ë‹¹: {format_bytes(total_per_layer)}")
                print(f"       ì „ì²´:     {format_bytes(total_all_layers)}")
    
    # Decoding deltas ì •ë³´
    if 'decoding_deltas' in data:
        deltas = data['decoding_deltas']
        print(f"\nğŸ”¢ Decoding Steps:")
        print(f"   ì´ ìŠ¤í… ìˆ˜: {len(deltas)}")
        
        if len(deltas) > 0:
            # ì²˜ìŒ 5ê°œ ìŠ¤í… ì¶œë ¥
            print(f"\n   ì²˜ìŒ 5ê°œ ìŠ¤í…:")
            for i, step_data in enumerate(deltas[:5]):
                token_id = step_data.get('token_id', 'N/A')
                token_text = step_data.get('token_text', 'N/A')
                # ê°œí–‰ ë¬¸ì í‘œì‹œ
                display_text = token_text.replace('\n', '\\n')
                print(f"     Step {i}: token_id={token_id:6d}, text='{display_text}'")
            
            if len(deltas) > 5:
                print(f"     ... ({len(deltas) - 5}ê°œ ë”)")
            
            # Delta í¬ê¸° ì •ë³´
            first_delta = deltas[0]
            if 'kv_delta' in first_delta:
                kv_delta = first_delta['kv_delta']
                print(f"\n   Delta KV Cache êµ¬ì¡°:")
                print(f"     ë ˆì´ì–´ ìˆ˜: {len(kv_delta)}")
                
                if len(kv_delta) > 0:
                    keys, values = kv_delta[0]
                    print(f"     ê° Delta (1 í† í°):")
                    print(f"       Keys:   {format_tensor_info(keys)}")
                    print(f"       Values: {format_tensor_info(values)}")
                    
                    # Delta ë©”ëª¨ë¦¬
                    key_size = keys.element_size() * keys.nelement()
                    value_size = values.element_size() * values.nelement()
                    total_per_delta = (key_size + value_size) * len(kv_delta)
                    
                    print(f"     Delta ë©”ëª¨ë¦¬ (ë ˆì´ì–´ë‹¹):")
                    print(f"       1ê°œ Delta: {format_bytes(total_per_delta)}")
                    print(f"       ì „ì²´ {len(deltas)}ê°œ: {format_bytes(total_per_delta * len(deltas))}")
            
            # ë§ˆì§€ë§‰ ìŠ¤í…ì˜ cumulative length
            last_step = deltas[-1]
            if 'cumulative_seq_length' in last_step:
                print(f"\n   ìµœì¢… ì‹œí€€ìŠ¤ ê¸¸ì´: {last_step['cumulative_seq_length']}")
    
    # Generation result
    if 'generation_result' in data:
        print(f"\nâœ¨ ìƒì„± ê²°ê³¼:")
        result = data['generation_result']
        
        if 'generated_text' in result:
            text = result['generated_text']
            preview = text[:200] + "..." if len(text) > 200 else text
            print(f"   ìƒì„±ëœ í…ìŠ¤íŠ¸: {preview}")
        
        if 'generated_token_ids' in result:
            tokens = result['generated_token_ids']
            print(f"   ìƒì„±ëœ í† í° ìˆ˜: {len(tokens)}")
            print(f"   í† í° ID (ì²˜ìŒ 20ê°œ): {tokens[:20]}")
        
        if 'total_steps' in result:
            print(f"   ì´ ìŠ¤í… ìˆ˜: {result['total_steps']}")
    
    # ì „ì²´ íŒŒì¼ êµ¬ì¡° ìš”ì•½
    print(f"\nğŸ“Š ì „ì²´ êµ¬ì¡° ìš”ì•½:")
    total_memory = 0
    
    if 'prefill_kv' in data and 'past_key_values' in data['prefill_kv']:
        kv = data['prefill_kv']['past_key_values']
        if len(kv) > 0:
            keys, values = kv[0]
            prefill_size = (keys.element_size() * keys.nelement() + 
                           values.element_size() * values.nelement()) * len(kv)
            total_memory += prefill_size
            print(f"   Prefill KV Cache: {format_bytes(prefill_size)}")
    
    if 'decoding_deltas' in data:
        deltas = data['decoding_deltas']
        if len(deltas) > 0 and 'kv_delta' in deltas[0]:
            kv_delta = deltas[0]['kv_delta']
            if len(kv_delta) > 0:
                keys, values = kv_delta[0]
                delta_size = (keys.element_size() * keys.nelement() + 
                             values.element_size() * values.nelement()) * len(kv_delta)
                total_deltas_size = delta_size * len(deltas)
                total_memory += total_deltas_size
                print(f"   Decoding Deltas ({len(deltas)}ê°œ): {format_bytes(total_deltas_size)}")
    
    print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"   ì „ì²´ KV Cache: {format_bytes(total_memory)}")
    print(f"   íŒŒì¼ í¬ê¸°:     {format_bytes(file_size)}")
    overhead = file_size - total_memory
    print(f"   ì˜¤ë²„í—¤ë“œ:      {format_bytes(overhead)} ({100*overhead/file_size:.1f}%)")
    
    print(f"\n" + "="*80)
    print("ì¡°íšŒ ì™„ë£Œ")
    print("="*80)


if __name__ == "__main__":
    # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ
    default_path = "/home/work/easyhyum/xpu-character-test/outputs/20251125-144134/kv_caches/NVIDIA H200/redhatai_meta_llama_3.1_8b_fp8/batch16_input0.pt"
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ íŒŒì¼ ê²½ë¡œ ë°›ê¸°
    if len(sys.argv) > 1:
        filepath = sys.argv[1]
    else:
        filepath = default_path
    
    inspect_kv_cache(filepath)
